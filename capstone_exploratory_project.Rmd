---
title: "Data Science Capstone"
author: "jbratanov"
date: "Monday, August 22, 2016"
output: html_document
---
# Synopsis
We will be doing Exploratory data analysis of data from SwiftKey, a company which is known for their application on cell phones that learn your word patterns as you type messages.  Our goal is to build a similar predictive text pattern algorithm as the Swiftkey solution using Natural Language Processing to predict the next word or set of words as users type real time.

# Dataset Processing

### Dataset Information
The data is from a corpus called HC Corpora and is located at www.corpora.heliohost.org.  The readme file for the dataset used is located at www.corpora.heliohost.org/aboutcorpus.html.

### Dataset Download Information
In order to speed up retrieval of the training data to be used by the Capstone project, I manually downloaded the training data located at https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip.

The en_us based datasets were unzipped into the capstone project data directory as: 

+ en_US.news.txt
+ en_US.twitter.txt
+ en_US.blogs.txt


```{r cacheDataRead, cache=TRUE}


# Load files using cache option, as re-reading during test would take too long
# Included skilNul=TRUE statement to get rid of nulls from first pass.
blogText <- readLines("../data/en_US.blogs.txt", skipNul = TRUE)
newsText <- readLines("../data/en_US.news.txt", skipNul = TRUE)
twitterText <- readLines("../data/en_US.twitter.txt", skipNul = TRUE)

```
### Initial Dataset Analysis
#### Display and describe first few lines of Information in each Dataset

   + __Blog__ - The text is informal with decent grammar, but tends to run on too long without breaks.  Observed non-English words and special characters.
   + __News__ - The text is formal and describes events with good grammar.  Initial observations show no special characters outside of punctuation.
   + __Twitter__ - The data is very curt, informal and poor grammar.  Sentences are brief with jargon and many special characters.

```{r cacheDataRead2, cache=TRUE, echo=FALSE}
library(stringr)

# Get file size data
blogFileSize <- file.info("../data/en_US.blogs.txt")$size
newsFileSize <- file.info("../data/en_US.news.txt")$size
twitFileSize <- file.info("../data/en_US.twitter.txt")$size

# Show first few rows of data
head(blogText, 4)
head(newsText, 4)
head(twitterText, 4)

# Get number of lines
blogLen <- length(blogText)
newsLen <- length(newsText)
twitLen <- length(twitterText)

# Get number of words
# Regex \\s+ is used to split on whitespace
blogWords <- sum(sapply(strsplit(blogText, "\\s+"), length))
newsWords <- sum(sapply(strsplit(newsText, "\\s+"), length))
twitWords <- sum(sapply(strsplit(twitterText, "\\s+"), length))

# Get number of characters
blogSum <- sum(nchar(blogText))
newsSum <- sum(nchar(newsText))
twitSum <- sum(nchar(twitterText))

# Create DF to store info
name <- c('Blog', 'News', 'Twitter')
fileSize <- c(blogFileSize, newsFileSize, twitFileSize)
lines <- c(blogLen, newsLen, twitLen)
words <- c(blogWords, newsWords, twitWords)
chars <- c(blogSum, newsSum, twitSum)
avgLineSize <- c(as.integer(blogSum/blogLen), as.integer(newsSum/newsLen), as.integer(twitSum/twitLen))
aveWordsPerLine <- c(as.integer(blogWords/blogLen),
                     as.integer(newsWords/newsLen), as.integer(twitWords/twitLen))
textInfo <- data.frame(name, fileSize, lines, words, chars, avgLineSize, aveWordsPerLine)
```
#### Table Grid with Dataset Summary Information
I was able to use the Dataset summary information to detect a problem with loading all the data from the News Text file.  The Blog and Twitter datasets were 99% and 97% respectively between file byte size and number characters read, the News Dataset came in at 75%.  Although the data loss was 25% of the file, I decided to move forward as the amount of data captured serves the purpose for this exercise.  I did a visual and print of the data and saw no extraneous or bad data which would affect the project.
```{r cacheDataRead3, cache=TRUE, echo=FALSE}

# Create a summary grid of data
library(gridExtra)
library(grid)

# Get grid format
gridTheme <- ttheme_default(core=list(
        bg_params = list(fill=c("yellow", "red", "green"))
))
    
# Create Data Grid
grid.table(textInfo, theme=gridTheme)



```

#### Barplots to Visually Compare Dataset Information
The metric information for the datasets supports the visual observations of the data.  Twitter average words per line is one third of news and blog lines.  Blogs as expected, is the largest.

```{r cacheDataRead4, cache=TRUE, echo=TRUE}

par(mfrow=c(2,2))


# Number of Lines Plot
par(cex=.5)
plotLines <- barplot(textInfo$lines/1000, main="Number of Lines in (Thousands)",
                     horiz=TRUE, names.arg=textInfo$name, col=c("yellow", "red", "green"),
                     ylab="Corpus Source", cex.lab=1, cex.main=2, xlim=c(0, 2500))
text(textInfo$lines/1000+55, 1:3, as.integer(textInfo$lines/1000), cex=1)

# Number of Words Plot
par(cex=.5)
plotLines <- barplot(textInfo$words/1000000, main="Number of Words in (Millions)",
                     horiz=TRUE, names.arg=textInfo$name, col=c("yellow", "red", "green"),
                     ylab="Corpus Source", cex.lab=1, cex.main=2, xlim=c(0, 40))
text(textInfo$words/1000000+1, 1:3, as.integer(textInfo$words/1000000), cex=1)

# Number of characters Plot
par(cex=.5)
plotLines <- barplot(textInfo$chars/1000000, main="Number of Characters in (Millions)",
                     horiz=TRUE, names.arg=textInfo$name, col=c("yellow", "red", "green"),
                     ylab="Corpus Source", cex.lab=1, cex.main=2, xlim=c(0, 300))
text(textInfo$chars/1000000+10, 1:3, as.integer(textInfo$chars/1000000), cex=1)

# Average line size Plot
par(cex=.5)
plotLines <- barplot(as.integer(textInfo$avgLineSize), main="Average Line Size",
                     horiz=TRUE, names.arg=textInfo$name, col=c("yellow", "red", "green"),
                     ylab="Corpus Source", cex.lab=1, cex.main=2, xlim=c(0, 300))
text(as.integer(textInfo$avgLineSize)+10, 1:3, as.integer(textInfo$avgLineSize), cex=1)



```
# Data Exploratory Analysis
Now that we understand the high level and physical characters of the datasets, we will start analyzing the details of the data.  I decided to treat each dataset as its own.  There were conflicting recommendations on merging versus not merging datasets.  I felt each corpus domain (blogs, news, tweets) had its own challenges which could be better solved individually.  Based on initial data analysis, we will determine if merging files for the project makes sense.

After running into memory issues with many conversion functions, I ended up going from initially trying 50% down to 1% of data samples to do data exploratory.

As I started to work with the Corpus data, I had to start eliminating types of characters until I was able to get a good Corpus file of alpha words.  I ended up iteratively removing different character types.

+ Converting all to lower case, so simlar spelled words weren't affected by alpha case (lower/higher)
+ Striping white spaces from words
+ Removed punctuation which caused words to not be recognized
+ Removed numbers.  They seemed to be extranous information mixed with words



```{r cacheDataRead5, cache=TRUE, echo=TRUE}
library(tm)

sampleBlog <- sample(blogText,length(blogText)*0.001)
sampleNews <- sample(newsText,length(newsText)*0.01)
sampleTwit <- sample(twitterText,length(twitterText)*0.001)

# Create corpus data structures
blogCorpus <- Corpus(VectorSource(sampleBlog))
newsCorpus <- Corpus(VectorSource(sampleNews))
twitCorpus <- Corpus(VectorSource(sampleTwit))

# Convert to lower, so words with different cases align
blogCorpus <- tm_map(blogCorpus, content_transformer(tolower))
newsCorpus <- tm_map(newsCorpus, content_transformer(tolower))
twitCorpus <- tm_map(twitCorpus, content_transformer(tolower))

# Need to remove the white space from the words
blogCorpus <- tm_map(blogCorpus, content_transformer(stripWhitespace))
newsCorpus <- tm_map(newsCorpus, content_transformer(stripWhitespace))
twitCorpus <- tm_map(twitCorpus, content_transformer(stripWhitespace))

# Remove Punctuation
blogCorpus <- tm_map(blogCorpus, removePunctuation)
newsCorpus <- tm_map(newsCorpus, removePunctuation)
twitCorpus <- tm_map(twitCorpus, removePunctuation)

# Remove numbers
blogCorpus <- tm_map(blogCorpus, removeNumbers)
newsCorpus <- tm_map(newsCorpus, removeNumbers)
twitCorpus <- tm_map(twitCorpus, removeNumbers)


# Create the doc-term matrix
dtmBlog <- DocumentTermMatrix(blogCorpus)
dtmNews <- DocumentTermMatrix(newsCorpus)
dtmTwit <- DocumentTermMatrix(twitCorpus)

## Sort the word frequencies and print top 15.
matrixBlog <- as.matrix(dtmBlog)
blogFreq <- colSums(matrixBlog)
matrixBlogSort <- sort(blogFreq, decreasing = TRUE)[1:15]
barplot(matrixBlogSort, xlab = "Words", ylab = "Frequency", main = "Top 15 frequent words in Blog Dataset")

matrixNews <- as.matrix(dtmNews)
newsFreq <- colSums(matrixNews)
matrixNewsSort <- sort(newsFreq, decreasing = TRUE)[1:15]
barplot(matrixNewsSort, xlab = "Words", ylab = "Frequency", main = "Top 15 frequent words in News Dataset")

matrixTwit <- as.matrix(dtmTwit)
twitFreq <- colSums(matrixTwit)
matrixTwitSort <- sort(twitFreq, decreasing = TRUE)[1:15]
barplot(matrixTwitSort, xlab = "Words", ylab = "Frequency",
        main = "Top 15 frequent words in Twitter Dataset")

```

### Next Steps
First we will need to do some additional clean-up of the data to allow further investigation of frequent words and phrases for each dataset.  Based on recommendations from R blogs, I will use the TM package to clean up data through its tranformation functions.  Although many task could be accomplished through R strings packages, this will give me a better insight into how TM works.  Also, many of the functions used in TM have short cuts which would require much more code in R.

+ Clean R code up to be more compact.  I repeated many repeated functions by text datasets to allow for easy trial and error between the three different data sources (blogs, news, twitter)

+ With memory resource challenges, I will need to look at ways for modeling the data

+ Run n-gram models for 2 and 3 n-grams phrase algorithm to understand what phrases can be easily predicted versus rarely or not used.

+ Based on -gram results, look at possible deletion of words rarely used.  Many sites include recommendation of removing and not allowing profanity which I will investigate

+ Need to determine if the model should work the same for all text types or use one common mode.  Will use deeper text analysis to determine which makes sense.




